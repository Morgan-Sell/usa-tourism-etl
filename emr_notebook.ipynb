{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "inner-delay",
   "metadata": {},
   "source": [
    "# ETL Notebook\n",
    "\n",
    "Execute `etl.py` code in a notebook because EMR Notebook instance will not allow the running of a script in its terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "environmental-fancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e383957fc1ca4667a444c06333da3a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1619729626328_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-2-140.us-west-2.compute.internal:20888/proxy/application_1619729626328_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-2-130.us-west-2.compute.internal:8042/node/containerlogs/container_1619729626328_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id, row_number\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import TimestampType, DateType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "divine-proposal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318a8959662142d7babaa7c884036a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\" \n",
    "    Create spark entry point\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "            .getOrCreate()\n",
    "    \n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stable-emphasis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6ebd076fd24a23b8a133d085c3d94f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_airports_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Loads and processes the raw airport data using Spark. \n",
    "    Returns the data as a semi-schematized parquet file.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df = spark.read.option(\"header\", True).csv(input_data)\n",
    "\n",
    "    # Extract values from within columns\n",
    "    lat_long = F.split(df.coordinates, \",\")\n",
    "    df = df.withColumn(\"longitude\", lat_long.getItem(0))\n",
    "    df = df.withColumn(\"latitude\", lat_long.getItem(1))\n",
    "    region_split = F.split(df.iso_region, \"-\")\n",
    "    df = df.withColumn(\"state\", region_split.getItem(1))\n",
    "\n",
    "    # Select subset of original dataframe.\n",
    "    df2 = df.select([\"ident\",\n",
    "            \"iata_code\",\n",
    "            \"name\",\n",
    "            \"type\",\n",
    "            \"municipality\",\n",
    "            \"state\",\n",
    "            \"local_code\",\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "            \"elevation_ft\"]).where(df.iso_country==\"US\")\n",
    "    \n",
    "    # Revise numeric values data types\n",
    "    df2 = df2.withColumn(\"latitude\", df2.latitude.cast('float')) \\\n",
    "            .withColumn(\"longitude\", df2.longitude.cast('float')) \\\n",
    "            .withColumn(\"elevation_fit\", df2.elevation_ft.cast('integer')) \\\n",
    "            .withColumnRenamed(\"ident\", \"icao_code\") \\\n",
    "            .withColumnRenamed(\"local_code\", \"faa_code\")\n",
    "    \n",
    "    # Sort \n",
    "    df2 = df2.orderBy([\"state\", \"iata_code\", \"icao_code\"])\n",
    "    \n",
    "    # Export data to a parquet file\n",
    "    df2.write.partitionBy(\"state\").mode('overwrite').parquet(os.path.join(output_data, \"airports\"))\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "british-blowing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060c34d372564d5fadb8b7fc5cb17b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_cities_demographics_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Loads and processes the raw U.S. cities demographics data using Spark. \n",
    "    Returns the data as a semi-schematized parquet file.\n",
    "\n",
    "    \"\"\"\n",
    "    df = spark.read.option('header', True) \\\n",
    "                .option('delimiter', \";\") \\\n",
    "                .csv(input_data)\n",
    "    \n",
    "    df2 = df\n",
    "\n",
    "    for original, revised in config.USA_CITIES_RENAME_COLS.items():\n",
    "        df2 = df2.withColumnRenamed(original, revised)\n",
    "\n",
    "    df2 = df2.withColumn(\"state_city\", F.concat_ws(\"_\", df2.state_code, df2.city))\n",
    "\n",
    "    # Change data types to integers\n",
    "    for i_var in config.USA_CITIES_INTEGER_VARS:\n",
    "        df2 = df2.withColumn(i_var, df2[i_var].cast('integer'))\n",
    "    \n",
    "    # Change data types to floats\n",
    "    for f_var in config.USA_CITIES_FLOAT_VARS:\n",
    "        df2 = df2.withColumn(f_var, df2[f_var].cast('float'))\n",
    "\n",
    "    df2 = df2.dropDuplicates([\"state_city\"])\n",
    "\n",
    "    # Create race population group-by table.\n",
    "    race  = df2.select(\"state_city\", \"race\", \"race_pop\")\n",
    "    race = race.groupBy(\"state_city\").pivot(\"race\").agg(F.first(\"race_pop\"))\n",
    "\n",
    "    # join dataframes\n",
    "    df3 = df2.join(race, df2.state_city == race.state_city)\n",
    "    df3 = df3.drop(\"race\", \"race_pop\", \"state_city\", \"state_city\")\n",
    "             \n",
    "    for original, revised in config.RACE_RENAME_COLS.items():\n",
    "        df3 = df3.withColumnRenamed(original, revised)\n",
    "\n",
    "    df3 = df3.orderBy([\"state\", \"city\"])\n",
    "\n",
    "    # Export data to a parquet file\n",
    "    df3.write.partitionBy(\"state\").mode('overwrite').parquet(os.path.join(output_data, \"cities_demographics\"))\n",
    "    \n",
    "    return df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wrapped-spring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acb7b3d73b843c9aeef79cf99184d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_usa_temperature_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Loads global temperature files. \n",
    "    Returns a parquet file for the climate of U.S. cities.\n",
    "    \"\"\"\n",
    "\n",
    "    df = spark.read.option('header', True).csv(input_data)\n",
    "    df2 = df.select(\"*\").where((df.Country == \"United States\") & (df.dt > \"1969-12-31\"))\n",
    "    \n",
    "    for original, revised in config.TEMPERATURE_RENAME_COLS.items():\n",
    "        df2 = df2.withColumnRenamed(original, revised)\n",
    "    \n",
    "    df2 = df2.withColumn(\"lat_length\", F.length(\"latitude\")) \\\n",
    "            .withColumn(\"long_length\", F.length(\"longitude\")) \\\n",
    "            .withColumn(\"latitude_2\", F.expr(\"\"\"substr(latitude, 1, lat_length-1)\"\"\")) \\\n",
    "            .withColumn(\"longitude_2\", F.expr(\"\"\"substr(longitude, 1, long_length-1)\"\"\"))\n",
    "    \n",
    "    df2 = df2.withColumn(\"latitude\", df2.latitude_2.cast('float')) \\\n",
    "            .withColumn(\"longitude\", df2.longitude_2.cast('float'))\n",
    "\n",
    "    df2 = df2.withColumn(\"longitude\", -1 * col(\"longitude\")) \\\n",
    "            .withColumn(\"year\", year(df2.date)) \\\n",
    "            .withColumn(\"month\", month(df2.date)) \\\n",
    "            .drop(\"Country\", \"lat_length\", \"long_length\", \"latitude_2\", \"longitude_2\")\n",
    "    \n",
    "    df2 = df2.orderBy([\"date\", \"city\"])\n",
    "\n",
    "    df2.write.partitionBy(\"year\").mode('overwrite').parquet(os.path.join(output_data, \"usa_temperatures\"))\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "featured-district",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2424a5d58241afa004307371c953f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_datetime(num_days):\n",
    "    \"\"\"\n",
    "    Converts a uni-codic numeric string value to a date object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start = datetime(1960, 1, 1)\n",
    "        res = start + timedelta(days=int(float((num_days))))\n",
    "        return res.date()\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "structured-capability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fc3c88bdb54c4d806f9575a4b2d261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_usa_tourism_data(spark, tourism_data, airport_codes, country_codes, output_data):\n",
    "    \"\"\"\n",
    "    Loads and process the U.S. tourism SAS files.\n",
    "    Joins tourism data with airport_codes and countries.\n",
    "    Returns PySpark dataframe as partitioned parquet files.\n",
    "    \"\"\"\n",
    "\n",
    "    tourism = spark.read.parquet(tourism_data)\n",
    "    \n",
    "    airports = spark.read.option('header', True).csv(airport_codes)\n",
    "    countries = spark.read.option('header', True).csv(country_codes)\n",
    "\n",
    "    # Create airport-cities dictionary\n",
    "    airports2 = airports.withColumn(\"city\", F.split(col(\"airport\"), \",\").getItem(0))\n",
    "    airports2 = airports2.withColumn(\"city\", F.initcap(\"city\")) \\\n",
    "                        .drop(\"airport\")\n",
    "    \n",
    "    # Create country-I94 code dictionary\n",
    "    udf_datetime_from_sas = udf(lambda x: convert_datetime(x), DateType())\n",
    "    countries2 = countries.withColumn(\"country\", F.initcap(\"country\")) \\\n",
    "                        .withColumn(\"country_code\", countries.country_code.cast('integer'))\n",
    "\n",
    "    # Process tourism data\n",
    "    udf_datetime_from_sas = udf(lambda x: convert_datetime(x), DateType())\n",
    "\n",
    "    #tourism2 = tourism.withColumn(\"arrival_date\", udf_datetime_from_sas(col(\"arrdate\"))) \\\n",
    "    tourism2 = tourism.withColumn(\"arrival_date\", udf_datetime_from_sas(tourism.arrdate)) \\\n",
    "                .withColumn(\"departure_date\", udf_datetime_from_sas(tourism.depdate)) \\\n",
    "                .drop(*config.DROP_TOURISM_COLS)\n",
    "    \n",
    "    for original, renamed in config.TOURISM_RENAME_COLS.items():\n",
    "        tourism2 = tourism2.withColumnRenamed(original, renamed)\n",
    "\n",
    "    for feature in config.TOURISM_INTEGER_VARS:\n",
    "        tourism2 = tourism2.withColumn(feature, tourism2[feature].cast('integer'))\n",
    "    \n",
    "    # Create master dataframe by joining tourism2 and countries2 dataframes.\n",
    "    master = tourism2.join(countries2,\n",
    "                    tourism2.citizen_cntry_code == countries2.country_code,\n",
    "                    how ='left')\n",
    "    \n",
    "    master = master.withColumnRenamed(\"country\", \"citizen_country\") \\\n",
    "                    .drop(\"country_code\")\n",
    "    \n",
    "    master = master.join(countries2,\n",
    "                    master.residency_cntry_code == countries2.country_code,\n",
    "                    how='left')\n",
    "    \n",
    "    master = master.withColumnRenamed(\"country\", \"residency_country\") \\\n",
    "                    .drop(\"country_code\")\n",
    "    \n",
    "    # Join master and airports2 dataframes.\n",
    "    master = master.join(airports2, master.airport == airports2.airport_code, how='left')\n",
    "    master = master.withColumnRenamed(\"city\", \"airport_city\") \\\n",
    "                    .drop(\"airport_code\")\n",
    "\n",
    "    # Change the categorical values from integers/characters to descriptive strings.\n",
    "    travel_mode_func = udf(lambda x: config.MODE_OF_TRAVEL.get(x), StringType())\n",
    "    travel_reason_func = udf(lambda x: config.REASON_FOR_TRAVEL.get(x), StringType())\n",
    "    maritime_signals_func = udf(lambda x: config.MARITIME_SIGNAL_FLAGS.get(x), StringType())\n",
    "\n",
    "    master = master.withColumn(\"travel_mode\", travel_mode_func(master.travel_mode)) \\\n",
    "                        .withColumn(\"reason_for_travel\", travel_reason_func(master.reason_for_travel)) \\\n",
    "                        .withColumn(\"maritime_status_arrival\", maritime_signals_func(master.entdepa)) \\\n",
    "                        .withColumn(\"maritime_status_departure\", maritime_signals_func(master.entdepd)) \\\n",
    "                        .drop(\"entedepa\", \"entdepd\")\n",
    "    \n",
    "    # Add sequential ID\n",
    "    master = master.orderBy(\"arrival_date\") \\\n",
    "                .withColumn(\"tourism_id\", monotonically_increasing_id())\n",
    "    master = master.orderBy(\"tourism_id\")\n",
    "    #window = Window.orderBy(col(\"mono_increasing_id\"))\n",
    "\n",
    "    # Write master dataframe to parque files partitioned by year and month\n",
    "    master.write.partitionBy(\"arrival_yr\", \"arrival_month\").mode('overwrite').parquet(os.path.join(output_data, \"tourist_visits\"))\n",
    "    \n",
    "    return master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "decent-processor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb18b95a6bd491595a43d3f178eb710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def check_for_successful_load(airports_url, cities_url, weather_url, tourism_url):\n",
    "    \"\"\"\n",
    "    Retrieves a sample of the data loaded into S3 to confirm data was successfully loaded.\n",
    "    \n",
    "    \"\"\"\n",
    "    airports_check = spark.read.parquet(airport_url)\n",
    "    cities_check = spark.read.parquet(cities_url)\n",
    "    weather_check = spark.read.parquet(weather_url)\n",
    "    tourism_check = spark.read.parquet(tourism_url)\n",
    "    \n",
    "    \n",
    "    if airports_check.count() == 0:\n",
    "        print(\"ERROR: Airport data was not properly loaded\")\n",
    "    \n",
    "    elif cities_check.count() == 0:\n",
    "        print(\"ERROR: Cities data was not properly loaded\")\n",
    "        \n",
    "    elif weather_check.count() == 0:\n",
    "        print(\"ERROR: Weather data was not properly loaded\")\n",
    "    \n",
    "    elif tourism_check.count() == 0:\n",
    "        print(\"ERROR: Tourism data was not properly loaded\")\n",
    "        \n",
    "    else:\n",
    "        print(\"All tables successfully loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-sessions",
   "metadata": {},
   "source": [
    "## Run ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "honest-portugal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464a44ad27f14814bdae789b03f586ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "otherwise-abortion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c3e3acd9b848d49d11f252b111afed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Update path whenever a new EMR cluster is created/used.\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.addPyFile(path=\"s3://aws-emr-resources-588336835903-us-west-2/notebooks/e-70ZSFY63SJ7TIRVLS0DQLQIN7/usa-tourism-etl/config.py\")\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIRPORT_DATA=\"s3a://udac-capstone/airports.csv\"\n",
    "TOURISM_DATA=\"s3a://udac-capstone/tourism_data/*.parquet\"\n",
    "USA_CITIES_DATA=\"s3a://udac-capstone/us_cities_demographics.csv\"\n",
    "WEATHER_DATA=\"s3a://udac-capstone/GlobalLandTemperaturesByCity.csv\"\n",
    "AIRPORT_CODES=\"s3a://udac-capstone/airport_codes.csv\"\n",
    "COUNTRY_CODES=\"s3a://udac-capstone/country_codes.csv\"\n",
    "OUTPUT_PATH = \"s3a://udac-capstone-output/\"\n",
    "\n",
    "airports_data = process_airports_data(spark, AIRPORT_DATA, OUTPUT_PATH)\n",
    "cities_data = process_cities_demographics_data(spark, USA_CITIES_DATA, OUTPUT_PATH)\n",
    "weather_data = process_usa_temperature_data(spark, WEATHER_DATA, OUTPUT_PATH)\n",
    "tourism_data = process_usa_tourism_data(spark, TOURISM_DATA, AIRPORT_CODES, COUNTRY_CODES, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-circle",
   "metadata": {},
   "source": [
    "## Data Quality Check #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "respected-pregnancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809a60c9a1e449909cb23aa3b41959e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tables successfully loaded!"
     ]
    }
   ],
   "source": [
    "# Confirm data was successfully loaded into an S3 bucket.\n",
    "airports_url = \"s3a://udac-capstone-output/airports/state=CA/*.parquet\"\n",
    "cities_url = \"s3a://udac-capstone-output/cities_demographics/state=California/*.parquet\"\n",
    "tourism_url = \"s3a://udac-capstone-output/tourist_visits/arrival_yr=2016/arrival_month=4/*.parquet\"\n",
    "weather_url = \"s3a://udac-capstone-output/usa_temperatures/year=1984/*.parquet\"\n",
    "\n",
    "check_for_successful_load(airports_url, cities_url, weather_url, tourism_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-pickup",
   "metadata": {},
   "source": [
    "## Data Quality Check #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "coupled-patrick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa91d99f079540f2807bc58443f76d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def check_files_data_types(url, check_arr):\n",
    "    \"\"\"\n",
    "    Confirms if the dataset loaded into S3 contains the correct data type.\n",
    "    \"\"\"\n",
    "    num_incorrect = 0\n",
    "    df = spark.read.parquet(weather_url)\n",
    "    \n",
    "    for actual, check in zip(df.dtypes, check_arr):\n",
    "        if actual[1] != check:\n",
    "            num_correct += 1\n",
    "    \n",
    "    if num_incorrect > 0:\n",
    "        print(\"Column data types do not reconcile.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Column data types reconcile!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "united-fruit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b778b2d0314dbaa2daef04f6e21d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column data types reconcile!"
     ]
    }
   ],
   "source": [
    "correct_types = ['string', 'string', 'string', 'string', 'float', 'float', 'int']\n",
    "check_files_data_types(weather_url, correct_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-birth",
   "metadata": {},
   "source": [
    "## Query #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-nursing",
   "metadata": {},
   "source": [
    "Where do the majority of tourists live?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "residency_count = tourism_data.where(tourism_data.residency_country.isNotNull()) \\\n",
    "                            .groupby(tourism_data.residency_country) \\\n",
    "                            .sum(\"num_people\") \\\n",
    "                            .select([col(\"residency_country\"), col(\"sum(num_people)\").alias(\"total_people\")]) \\\n",
    "                            .sort(col(\"total_people\").desc())\n",
    "\n",
    "residency_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-speaking",
   "metadata": {},
   "source": [
    "## Query #2\n",
    "\n",
    "Is there a correlation between the number of foreigners visiting and percentage of foreigners living the in that city?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_destination = tourism_data.where(col(\"airport_city\").isNotNull()) \\\n",
    "                            .groupby(col(\"airport_city\")) \\\n",
    "                            .sum(\"num_people\") \\\n",
    "                            .select([col(\"airport_city\"), col(\"sum(num_people)\").alias(\"total_visitors\")])\n",
    "\n",
    "\n",
    "prcnt_foreigners = cities_data.withColumn(\"prcnt_foreigners\", col(\"num_foreigners\") / col(\"total_pop\")) \\\n",
    "                            .select([col(\"city\"), F.round(col(\"prcnt_foreigners\"), 4).alias(\"prcnt_foreigners\")])\n",
    "\n",
    "\n",
    "cities_foreigners = city_destination.join(prcnt_foreigners, city_destination.airport_city == prcnt_foreigners.city, how='right') \\\n",
    "                                    .sort(col(\"total_visitors\").desc()) \\\n",
    "                                    .select(col(\"city\"), col(\"total_visitors\"), col(\"prcnt_foreigners\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_foreigners.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_foreigners.stat.corr(\"total_visitors\", \"prcnt_foreigners\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
