{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "miniature-nitrogen",
   "metadata": {},
   "source": [
    "# Spark Local Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "electoral-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import configparser\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id, row_number\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import TimestampType, DateType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-petroleum",
   "metadata": {},
   "source": [
    "## Configure Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dominant-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "copyrighted-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = \"test_output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "selective-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure = SparkConf().setAppName('udac_config').setMaster('local')\n",
    "sc = SparkContext(conf = configure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unnecessary-amsterdam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getOrCreate modifies the parameters of existing Spark Session\n",
    "spark = SparkSession.builder.appName('udac_cap').config('config option', 'config value').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "electric-auckland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.app.startTime', '1616538822367'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/Users/morgan/Documents/10_Udacity/data_eng_nano/usa-tourism-etl/spark-warehouse'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.host', '10.0.0.223'),\n",
       " ('spark.app.name', 'udac_config'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.port', '51287'),\n",
       " ('spark.app.id', 'local-1616538823268')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-rochester",
   "metadata": {},
   "source": [
    "### Airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "missing-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = spark.read.option(\"header\", True).csv(\"data/airport_codes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "southwest-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long = F.split(airports.coordinates, \",\")\n",
    "airports = airports.withColumn('longitude', lat_long.getItem(0))\n",
    "airports = airports.withColumn('latitude', lat_long.getItem(1))\n",
    "\n",
    "region_split = F.split(airports.iso_region, \"-\")\n",
    "airports = airports.withColumn('state', region_split.getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "anticipated-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = airports.select(['ident',\n",
    "                 'iata_code',\n",
    "                 'name','type',\n",
    "                 'municipality',\n",
    "                 'state',\n",
    "                 'local_code',\n",
    "                 'latitude',\n",
    "                 'longitude',\n",
    "                 'elevation_ft']).where(airports.iso_country==\"US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accurate-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = airports.sort('iata_code', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "embedded-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = airports.na.drop(subset='iata_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "worth-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = airports.withColumn(\"latitude\", airports.latitude.cast('float')) \\\n",
    "                    .withColumn(\"longitude\", airports.longitude.cast('float')) \\\n",
    "                    .withColumn(\"elevation_fit\", airports.elevation_ft.cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "endangered-childhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+--------------------+--------------+-------------+-----+----------+--------+---------+------------+-------------+\n",
      "|ident|iata_code|                name|          type| municipality|state|local_code|latitude|longitude|elevation_ft|elevation_fit|\n",
      "+-----+---------+--------------------+--------------+-------------+-----+----------+--------+---------+------------+-------------+\n",
      "| KAAF|      AAF|Apalachicola Regi...| small_airport| Apalachicola|   FL|       AAF| 29.7275| -85.0275|          20|           20|\n",
      "| KAAP|      AAP|      Andrau Airpark|        closed|      Houston|   TX|       AAP| 29.7225| -95.5883|          79|           79|\n",
      "| KABE|      ABE|Lehigh Valley Int...|medium_airport|    Allentown|   PA|       ABE| 40.6521| -75.4408|         393|          393|\n",
      "| KABI|      ABI|Abilene Regional ...|medium_airport|      Abilene|   TX|       ABI| 32.4113| -99.6819|        1791|         1791|\n",
      "| PAFM|      ABL|      Ambler Airport|medium_airport|       Ambler|   AK|       AFM| 67.1063| -157.857|         334|          334|\n",
      "| KABQ|      ABQ|Albuquerque Inter...| large_airport|  Albuquerque|   NM|       ABQ| 35.0402| -106.609|        5355|         5355|\n",
      "| KABR|      ABR|Aberdeen Regional...|medium_airport|     Aberdeen|   SD|       ABR| 45.4491| -98.4218|        1302|         1302|\n",
      "| KABY|      ABY|Southwest Georgia...|medium_airport|       Albany|   GA|       ABY| 31.5355| -84.1945|         197|          197|\n",
      "| KACB|      ACB|Antrim County Air...| small_airport|     Bellaire|   MI|       ACB| 44.9886| -85.1984|         623|          623|\n",
      "| KACK|      ACK|Nantucket Memoria...|medium_airport|    Nantucket|   MA|       ACK| 41.2531| -70.0602|          47|           47|\n",
      "| KACT|      ACT|Waco Regional Air...|medium_airport|         Waco|   TX|       ACT| 31.6113| -97.2305|         516|          516|\n",
      "| KACV|      ACV|California Redwoo...|medium_airport|Arcata/Eureka|   CA|       ACV| 40.9781| -124.109|         221|          221|\n",
      "| KACY|      ACY|Atlantic City Int...|medium_airport|Atlantic City|   NJ|       ACY| 39.4576| -74.5772|          75|           75|\n",
      "| KADG|      ADG|Lenawee County Ai...| small_airport|       Adrian|   MI|       ADG| 41.8677| -84.0773|         798|          798|\n",
      "| PADK|      ADK|        Adak Airport|medium_airport|  Adak Island|   AK|       ADK|  51.878| -176.646|          18|           18|\n",
      "| KADM|      ADM|Ardmore Municipal...| small_airport|      Ardmore|   OK|       ADM|34.30301|-97.01963|         777|          777|\n",
      "| PADQ|      ADQ|      Kodiak Airport|medium_airport|       Kodiak|   AK|       ADQ|   57.75| -152.494|          78|           78|\n",
      "| KPHH|      ADR|Robert F Swinnie ...| small_airport|      Andrews|   SC|       PHH| 33.4517| -79.5262|          26|           26|\n",
      "| KADS|      ADS|     Addison Airport| small_airport|       Dallas|   TX|       ADS| 32.9686| -96.8364|         644|          644|\n",
      "| KADH|      ADT|Ada Regional Airport| small_airport|          Ada|   OK|       ADH| 34.8043| -96.6713|        1016|         1016|\n",
      "+-----+---------+--------------------+--------------+-------------+-----+----------+--------+---------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-reminder",
   "metadata": {},
   "source": [
    "#### Successfully created parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "awful-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "#airports.write.mode('overwrite').parquet(os.path.join(output_data, \"airports\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-spread",
   "metadata": {},
   "source": [
    "---\n",
    "## USA Cities Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "changed-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = spark.read.option('header', True) \\\n",
    "        .option('delimiter', \";\") \\\n",
    "        .csv(\"data/us_cities_demographics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "other-batch",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = cities.withColumnRenamed(\"City\", \"city\") \\\n",
    "        .withColumnRenamed(\"State\", \"state\") \\\n",
    "        .withColumnRenamed(\"Median Age\", \"median_age\") \\\n",
    "        .withColumnRenamed(\"Male Population\", \"male_pop\") \\\n",
    "        .withColumnRenamed(\"Female Population\", \"female_pop\") \\\n",
    "        .withColumnRenamed(\"Total Population\", \"total_pop\") \\\n",
    "        .withColumnRenamed(\"Number of Veterans\", \"num_veterans\") \\\n",
    "        .withColumnRenamed(\"Foreign-born\", \"num_foreigners\") \\\n",
    "        .withColumnRenamed(\"Average Household Size\", \"avg_household_size\") \\\n",
    "        .withColumnRenamed(\"State Code\", \"state_code\") \\\n",
    "        .withColumnRenamed(\"Race\", \"race\") \\\n",
    "        .withColumnRenamed(\"Count\", \"race_pop\")\n",
    "\n",
    "cities = cities.withColumn(\"state_city\", F.concat_ws(\"_\", cities.state_code, cities.city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "valuable-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_vars = [\"male_pop\", \"female_pop\", \"total_pop\", \"num_veterans\", \"num_foreigners\", \"race_pop\"]\n",
    "float_vars = [\"median_age\", \"avg_household_size\"]\n",
    "\n",
    "for i_var in integer_vars:\n",
    "    cities = cities.withColumn(i_var, cities[i_var].cast('integer'))\n",
    "    \n",
    "for f_var in float_vars:\n",
    "    cities = cities.withColumn(f_var, cities[f_var].cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "peaceful-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities2 = cities.dropDuplicates([\"state_city\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "supported-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_count = cities.select(\"state_city\", \"race\", \"race_pop\")\n",
    "race_count = race_count.withColumn(\"race_pop\", race_count.race_pop.cast('float'))\n",
    "race_count = race_count.groupBy(\"state_city\").pivot(\"race\").agg(F.first(\"race_pop\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "reverse-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_final = cities2.join(race_count, cities2.state_city == race_count.state_city)\n",
    "cities_final = cities_final.drop(\"race\", \"race_pop\", \"state_city\", \"state_city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "transsexual-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_final = cities_final.withColumnRenamed(\"American Indian and Alaska Native\", \"native_american_pop\") \\\n",
    "                            .withColumnRenamed(\"Asian\", \"asian_pop\") \\\n",
    "                            .withColumnRenamed(\"Black or African-American\", \"black_american_pop\") \\\n",
    "                            .withColumnRenamed(\"Hispanic or Latino\", \"hispanic_pop\") \\\n",
    "                            .withColumnRenamed(\"White\", \"white_pop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "irish-family",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: float (nullable = true)\n",
      " |-- male_pop: integer (nullable = true)\n",
      " |-- female_pop: integer (nullable = true)\n",
      " |-- total_pop: integer (nullable = true)\n",
      " |-- num_veterans: integer (nullable = true)\n",
      " |-- num_foreigners: integer (nullable = true)\n",
      " |-- avg_household_size: float (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- native_american_pop: float (nullable = true)\n",
      " |-- asian_pop: float (nullable = true)\n",
      " |-- black_american_pop: float (nullable = true)\n",
      " |-- hispanic_pop: float (nullable = true)\n",
      " |-- white_pop: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cities_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-talent",
   "metadata": {},
   "source": [
    "#### Successfully created parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "governmental-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cities_final.write.mode('overwrite').parquet(os.path.join(output_data, \"cities\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-potential",
   "metadata": {},
   "source": [
    "---\n",
    "## USA Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "narrative-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = spark.read.option('header', True) \\\n",
    "                .csv(\"data/GlobalLandTemperaturesByCity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "attended-opposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperatures.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "equivalent-significance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperatures.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "foster-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = temperatures.select(\"*\").where((temperatures.Country == \"United States\") & (temperatures.dt > \"1969-12-31\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "flush-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = temperatures.withColumnRenamed(\"dt\", \"date_time\") \\\n",
    "                            .withColumnRenamed(\"AverageTemperature\", \"avg_daily_temp\") \\\n",
    "                            .withColumnRenamed(\"AverageTemperatureUncertainty\", \"avg_temp_temp_uncertainty\") \\\n",
    "                            .withColumnRenamed(\"City\", \"city\") \\\n",
    "                            .withColumnRenamed(\"Latitude\", \"latitude\") \\\n",
    "                            .withColumnRenamed(\"Longitude\", \"longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "complimentary-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = temperatures.withColumn(\"lat_length\", F.length(\"latitude\")) \\\n",
    "                            .withColumn(\"long_length\", F.length(\"longitude\")) \\\n",
    "                            .withColumn(\"latitude_2\", F.expr(\"\"\"substr(latitude, 1, lat_length-1)\"\"\")) \\\n",
    "                            .withColumn(\"longitude_2\", F.expr(\"\"\"substr(longitude, 1, long_length-1)\"\"\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "lucky-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = temperatures.withColumn(\"latitude\", temperatures.latitude_2.cast('float')) \\\n",
    "                            .withColumn(\"longitude\", temperatures.longitude_2.cast('float'))\n",
    "\n",
    "temperatures = temperatures.withColumn(\"longitude\", -1 * col(\"longitude\"))\n",
    "\n",
    "temperatures = temperatures.drop(\"Country\", \"lat_length\", \"long_length\", \"latitude_2\", \"longitude_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "sexual-gambling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------------------------+-------+--------+---------+\n",
      "| date_time|avg_daily_temp|avg_temp_temp_uncertainty|   city|latitude|longitude|\n",
      "+----------+--------------+-------------------------+-------+--------+---------+\n",
      "|1970-01-01|         3.969|                    0.289|Abilene|   32.95|  -100.53|\n",
      "|1970-02-01|         8.463|                    0.177|Abilene|   32.95|  -100.53|\n",
      "+----------+--------------+-------------------------+-------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperatures.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "enclosed-parks",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#temperatures.write.mode('overwrite').parquet(os.path.join(output_data, \"temperatures\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-volunteer",
   "metadata": {},
   "source": [
    "---\n",
    "## Visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-bonus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "super-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = spark.read.option('header', True) \\\n",
    "            .option('delimiter', \",\") \\\n",
    "            .csv(\"data/immigration_data_sample.csv\")\n",
    "\n",
    "airports_dict = spark.read.option('header', True) \\\n",
    "                .csv(\"data/airport_dict.csv\")\n",
    "\n",
    "countries_dict = spark.read.option('header', True) \\\n",
    "                .csv(\"data/country_codes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "informational-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports2_dict = airports_dict.withColumn(\"city\", F.split(col(\"airport\"), \",\").getItem(0)) \\\n",
    "                                .withColumn(\"state\", F.split(col(\"airport\"), \",\").getItem(1))\n",
    "\n",
    "countries2_dict = countries_dict.withColumn(\"country\", F.initcap(\"country\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "precise-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "deleteWhitespaceUDF = udf(lambda s: s.replace(\" \", \"\"), StringType())\n",
    "deleteApostropheUDF = udf(lambda s: s.replace(\"''\", \"\"), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "heard-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports2_dict = airports2_dict.withColumn(\"state\", deleteWhitespaceUDF(\"state\"))\n",
    "airports2_dict = airports2_dict.withColumn(\"state\", deleteApostropheUDF(\"state\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "split-storage",
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 77, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 77, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 77, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"<ipython-input-72-2cc272bc0f94>\", line 1, in <lambda>\nAttributeError: 'NoneType' object has no attribute 'replace'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-63b13a1d8609>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mairports2_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 77, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 77, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"/Users/morgan/opt/anaconda3/envs/udac_cap/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 77, in <lambda>\n    return lambda *a: g(f(*a))\n  File \"<ipython-input-72-2cc272bc0f94>\", line 1, in <lambda>\nAttributeError: 'NoneType' object has no attribute 'replace'\n"
     ]
    }
   ],
   "source": [
    "airports2_dict.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "twenty-cisco",
   "metadata": {},
   "outputs": [],
   "source": [
    "visits2 = visits.drop(\"insnum\", \"dtadfile\", \"fltno\", 'i94bir', \"occup\", \"matflag\", \"admnum\", \"entdepu\", \"visapost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "empty-regulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- cicid: string (nullable = true)\n",
      " |-- i94yr: string (nullable = true)\n",
      " |-- i94mon: string (nullable = true)\n",
      " |-- i94cit: string (nullable = true)\n",
      " |-- i94res: string (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: string (nullable = true)\n",
      " |-- i94mode: string (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: string (nullable = true)\n",
      " |-- i94visa: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- biryear: string (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visits2 = visits2.withColumnRenamed(\"_c0\", \"visit_id\") \\\n",
    "            .withColumnRenamed(\"cicid\", \"citizen_id\") \\\n",
    "            .withColumnRenamed(\"i94yr\", \"arrival_yr\") \\\n",
    "            .withColumnRenamed(\"i94mon\", \"arrival_month\") \\\n",
    "            .withColumnRename(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = cities.withColumnRenamed(\"City\", \"city\") \\\n",
    "        .withColumnRenamed(\"State\", \"state\") \\\n",
    "        .withColumnRenamed(\"Median Age\", \"median_age\") \\\n",
    "        .withColumnRenamed(\"Male Population\", \"male_pop\") \\\n",
    "        .withColumnRenamed(\"Female Population\", \"female_pop\") \\\n",
    "        .withColumnRenamed(\"Total Population\", \"total_pop\") \\\n",
    "        .withColumnRenamed(\"Number of Veterans\", \"num_veterans\") \\\n",
    "        .withColumnRenamed(\"Foreign-born\", \"num_foreigners\") \\\n",
    "        .withColumnRenamed(\"Average Household Size\", \"avg_household_size\") \\\n",
    "        .withColumnRenamed(\"State Code\", \"state_code\") \\\n",
    "        .withColumnRenamed(\"Race\", \"race\") \\\n",
    "        .withColumnRenamed(\"Count\", \"race_pop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-incentive",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
