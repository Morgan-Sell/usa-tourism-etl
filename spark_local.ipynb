{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "loved-indonesian",
   "metadata": {},
   "source": [
    "# Spark Local Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "recent-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import configparser\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id, row_number\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import TimestampType, DateType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-documentation",
   "metadata": {},
   "source": [
    "## Configure Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sunrise-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stainless-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = \"test_output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "proof-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure = SparkConf().setAppName('udac_config').setMaster('local')\n",
    "sc = SparkContext(conf = configure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "banned-sessions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getOrCreate modifies the parameters of existing Spark Session\n",
    "spark = SparkSession.builder.appName('udac_cap').config('config option', 'config value').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "important-newman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.app.startTime', '1616538822367'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/Users/morgan/Documents/10_Udacity/data_eng_nano/usa-tourism-etl/spark-warehouse'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.host', '10.0.0.223'),\n",
       " ('spark.app.name', 'udac_config'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.port', '51287'),\n",
       " ('spark.app.id', 'local-1616538823268')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-contamination",
   "metadata": {},
   "source": [
    "### Airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "decreased-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = spark.read.option(\"header\", True).csv(\"data/airport_codes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "crucial-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long = F.split(airports.coordinates, \",\")\n",
    "airports = airports.withColumn('longitude', lat_long.getItem(0))\n",
    "airports = airports.withColumn('latitude', lat_long.getItem(1))\n",
    "\n",
    "region_split = F.split(airports.iso_region, \"-\")\n",
    "airports = airports.withColumn('state', region_split.getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "involved-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = airports.select(['ident',\n",
    "                 'iata_code',\n",
    "                 'name','type',\n",
    "                 'municipality',\n",
    "                 'state',\n",
    "                 'local_code',\n",
    "                 'latitude',\n",
    "                 'longitude',\n",
    "                 'elevation_ft']).where(airports.iso_country==\"US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "strange-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = airports.sort('iata_code', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "novel-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = airports.na.drop(subset='iata_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "conventional-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = airports.withColumn(\"latitude\", airports.latitude.cast('float')) \\\n",
    "                    .withColumn(\"longitude\", airports.longitude.cast('float')) \\\n",
    "                    .withColumn(\"elevation_fit\", airports.elevation_ft.cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "finite-lexington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+--------------------+--------------+-------------+-----+----------+--------+---------+------------+-------------+\n",
      "|ident|iata_code|                name|          type| municipality|state|local_code|latitude|longitude|elevation_ft|elevation_fit|\n",
      "+-----+---------+--------------------+--------------+-------------+-----+----------+--------+---------+------------+-------------+\n",
      "| KAAF|      AAF|Apalachicola Regi...| small_airport| Apalachicola|   FL|       AAF| 29.7275| -85.0275|          20|           20|\n",
      "| KAAP|      AAP|      Andrau Airpark|        closed|      Houston|   TX|       AAP| 29.7225| -95.5883|          79|           79|\n",
      "| KABE|      ABE|Lehigh Valley Int...|medium_airport|    Allentown|   PA|       ABE| 40.6521| -75.4408|         393|          393|\n",
      "| KABI|      ABI|Abilene Regional ...|medium_airport|      Abilene|   TX|       ABI| 32.4113| -99.6819|        1791|         1791|\n",
      "| PAFM|      ABL|      Ambler Airport|medium_airport|       Ambler|   AK|       AFM| 67.1063| -157.857|         334|          334|\n",
      "| KABQ|      ABQ|Albuquerque Inter...| large_airport|  Albuquerque|   NM|       ABQ| 35.0402| -106.609|        5355|         5355|\n",
      "| KABR|      ABR|Aberdeen Regional...|medium_airport|     Aberdeen|   SD|       ABR| 45.4491| -98.4218|        1302|         1302|\n",
      "| KABY|      ABY|Southwest Georgia...|medium_airport|       Albany|   GA|       ABY| 31.5355| -84.1945|         197|          197|\n",
      "| KACB|      ACB|Antrim County Air...| small_airport|     Bellaire|   MI|       ACB| 44.9886| -85.1984|         623|          623|\n",
      "| KACK|      ACK|Nantucket Memoria...|medium_airport|    Nantucket|   MA|       ACK| 41.2531| -70.0602|          47|           47|\n",
      "| KACT|      ACT|Waco Regional Air...|medium_airport|         Waco|   TX|       ACT| 31.6113| -97.2305|         516|          516|\n",
      "| KACV|      ACV|California Redwoo...|medium_airport|Arcata/Eureka|   CA|       ACV| 40.9781| -124.109|         221|          221|\n",
      "| KACY|      ACY|Atlantic City Int...|medium_airport|Atlantic City|   NJ|       ACY| 39.4576| -74.5772|          75|           75|\n",
      "| KADG|      ADG|Lenawee County Ai...| small_airport|       Adrian|   MI|       ADG| 41.8677| -84.0773|         798|          798|\n",
      "| PADK|      ADK|        Adak Airport|medium_airport|  Adak Island|   AK|       ADK|  51.878| -176.646|          18|           18|\n",
      "| KADM|      ADM|Ardmore Municipal...| small_airport|      Ardmore|   OK|       ADM|34.30301|-97.01963|         777|          777|\n",
      "| PADQ|      ADQ|      Kodiak Airport|medium_airport|       Kodiak|   AK|       ADQ|   57.75| -152.494|          78|           78|\n",
      "| KPHH|      ADR|Robert F Swinnie ...| small_airport|      Andrews|   SC|       PHH| 33.4517| -79.5262|          26|           26|\n",
      "| KADS|      ADS|     Addison Airport| small_airport|       Dallas|   TX|       ADS| 32.9686| -96.8364|         644|          644|\n",
      "| KADH|      ADT|Ada Regional Airport| small_airport|          Ada|   OK|       ADH| 34.8043| -96.6713|        1016|         1016|\n",
      "+-----+---------+--------------------+--------------+-------------+-----+----------+--------+---------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-pontiac",
   "metadata": {},
   "source": [
    "#### Successfully created parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "interstate-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "#airports.write.mode('overwrite').parquet(os.path.join(output_data, \"airports\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-performance",
   "metadata": {},
   "source": [
    "---\n",
    "## USA Cities Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aboriginal-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = spark.read.option('header', True) \\\n",
    "        .option('delimiter', \";\") \\\n",
    "        .csv(\"data/us_cities_demographics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "existing-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = cities.withColumnRenamed(\"City\", \"city\") \\\n",
    "        .withColumnRenamed(\"State\", \"state\") \\\n",
    "        .withColumnRenamed(\"Median Age\", \"median_age\") \\\n",
    "        .withColumnRenamed(\"Male Population\", \"male_pop\") \\\n",
    "        .withColumnRenamed(\"Female Population\", \"female_pop\") \\\n",
    "        .withColumnRenamed(\"Total Population\", \"total_pop\") \\\n",
    "        .withColumnRenamed(\"Number of Veterans\", \"num_veterans\") \\\n",
    "        .withColumnRenamed(\"Foreign-born\", \"num_foreigners\") \\\n",
    "        .withColumnRenamed(\"Average Household Size\", \"avg_household_size\") \\\n",
    "        .withColumnRenamed(\"State Code\", \"state_code\") \\\n",
    "        .withColumnRenamed(\"Race\", \"race\") \\\n",
    "        .withColumnRenamed(\"Count\", \"race_pop\")\n",
    "\n",
    "cities = cities.withColumn(\"state_city\", F.concat_ws(\"_\", cities.state_code, cities.city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "republican-yemen",
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_vars = [\"male_pop\", \"female_pop\", \"total_pop\", \"num_veterans\", \"num_foreigners\", \"race_pop\"]\n",
    "float_vars = [\"median_age\", \"avg_household_size\"]\n",
    "\n",
    "for i_var in integer_vars:\n",
    "    cities = cities.withColumn(i_var, cities[i_var].cast('integer'))\n",
    "    \n",
    "for f_var in float_vars:\n",
    "    cities = cities.withColumn(f_var, cities[f_var].cast('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cross-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities2 = cities.dropDuplicates([\"state_city\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "passive-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_count = cities.select(\"state_city\", \"race\", \"race_pop\")\n",
    "race_count = race_count.withColumn(\"race_pop\", race_count.race_pop.cast('float'))\n",
    "race_count = race_count.groupBy(\"state_city\").pivot(\"race\").agg(F.first(\"race_pop\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "promotional-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_final = cities2.join(race_count, cities2.state_city == race_count.state_city)\n",
    "cities_final = cities_final.drop(\"race\", \"race_pop\", \"state_city\", \"state_city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "saved-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_final = cities_final.withColumnRenamed(\"American Indian and Alaska Native\", \"native_american_pop\") \\\n",
    "                            .withColumnRenamed(\"Asian\", \"asian_pop\") \\\n",
    "                            .withColumnRenamed(\"Black or African-American\", \"black_american_pop\") \\\n",
    "                            .withColumnRenamed(\"Hispanic or Latino\", \"hispanic_pop\") \\\n",
    "                            .withColumnRenamed(\"White\", \"white_pop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "affiliated-respondent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: float (nullable = true)\n",
      " |-- male_pop: integer (nullable = true)\n",
      " |-- female_pop: integer (nullable = true)\n",
      " |-- total_pop: integer (nullable = true)\n",
      " |-- num_veterans: integer (nullable = true)\n",
      " |-- num_foreigners: integer (nullable = true)\n",
      " |-- avg_household_size: float (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- native_american_pop: float (nullable = true)\n",
      " |-- asian_pop: float (nullable = true)\n",
      " |-- black_american_pop: float (nullable = true)\n",
      " |-- hispanic_pop: float (nullable = true)\n",
      " |-- white_pop: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cities_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-daughter",
   "metadata": {},
   "source": [
    "#### Successfully created parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "lesbian-property",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cities_final.write.mode('overwrite').parquet(os.path.join(output_data, \"cities\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-default",
   "metadata": {},
   "source": [
    "---\n",
    "## USA Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "streaming-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = spark.read.option('header', True) \\\n",
    "                .csv(\"data/GlobalLandTemperaturesByCity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fatty-dollar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperatures.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "advised-falls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperatures.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "indonesian-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = temperatures.select(\"*\").where((temperatures.Country == \"United States\") & (temperatures.dt > \"1969-12-31\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "entitled-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = temperatures.withColumnRenamed(\"dt\", \"date_time\") \\\n",
    "                            .withColumnRenamed(\"AverageTemperature\", \"avg_daily_temp\") \\\n",
    "                            .withColumnRenamed(\"AverageTemperatureUncertainty\", \"avg_temp_temp_uncertainty\") \\\n",
    "                            .withColumnRenamed(\"City\", \"city\") \\\n",
    "                            .withColumnRenamed(\"Latitude\", \"latitude\") \\\n",
    "                            .withColumnRenamed(\"Longitude\", \"longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "nearby-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = temperatures.withColumn(\"lat_length\", F.length(\"latitude\")) \\\n",
    "                            .withColumn(\"long_length\", F.length(\"longitude\")) \\\n",
    "                            .withColumn(\"latitude_2\", F.expr(\"\"\"substr(latitude, 1, lat_length-1)\"\"\")) \\\n",
    "                            .withColumn(\"longitude_2\", F.expr(\"\"\"substr(longitude, 1, long_length-1)\"\"\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "italic-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = temperatures.withColumn(\"latitude\", temperatures.latitude_2.cast('float')) \\\n",
    "                            .withColumn(\"longitude\", temperatures.longitude_2.cast('float'))\n",
    "\n",
    "temperatures = temperatures.withColumn(\"longitude\", -1 * col(\"longitude\"))\n",
    "\n",
    "temperatures = temperatures.drop(\"Country\", \"lat_length\", \"long_length\", \"latitude_2\", \"longitude_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "greenhouse-right",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------------------------+-------+--------+---------+\n",
      "| date_time|avg_daily_temp|avg_temp_temp_uncertainty|   city|latitude|longitude|\n",
      "+----------+--------------+-------------------------+-------+--------+---------+\n",
      "|1970-01-01|         3.969|                    0.289|Abilene|   32.95|  -100.53|\n",
      "|1970-02-01|         8.463|                    0.177|Abilene|   32.95|  -100.53|\n",
      "+----------+--------------+-------------------------+-------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperatures.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ideal-security",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parquet() missing 1 required positional argument: 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-4b148a1f92cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# cities_final.write.mode('overwrite').parquet(os.path.join(output_data, \"cities\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtemperatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: parquet() missing 1 required positional argument: 'path'"
     ]
    }
   ],
   "source": [
    "# cities_final.write.mode('overwrite').parquet(os.path.join(output_data, \"cities\"))\n",
    "temperatures.write.mode('overwrite').parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-holocaust",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
