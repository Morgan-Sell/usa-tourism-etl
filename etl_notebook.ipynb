{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb8797ad",
   "metadata": {},
   "source": [
    "# ETL Notebook\n",
    "\n",
    "Execute `etl.py` code in a notebook because EMR Notebook instance will not allow the running of a script in its terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a058172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f065d5afee4055af8937fcbfac66a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1619452505109_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-2-248.us-west-2.compute.internal:20888/proxy/application_1619452505109_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-2-68.us-west-2.compute.internal:8042/node/containerlogs/container_1619452505109_0004_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id, row_number\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import TimestampType, DateType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aaebe57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f33c6804ede40c68870cbbc5c6bf6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\" \n",
    "    Create spark entry point\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "            .getOrCreate()\n",
    "    \n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e5fd335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9564eafcb0b9480ca3c5c6332fb340a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_airports_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Loads and processes the raw airport data using Spark. \n",
    "    Returns the data as a semi-schematized parquet file.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df = spark.read.option(\"header\", True).csv(input_data)\n",
    "\n",
    "    # Extract values from within columns\n",
    "    lat_long = F.split(df.coordinates, \",\")\n",
    "    df = df.withColumn(\"longitude\", lat_long.getItem(0))\n",
    "    df = df.withColumn(\"latitude\", lat_long.getItem(1))\n",
    "    region_split = F.split(df.iso_region, \"-\")\n",
    "    df = df.withColumn(\"state\", region_split.getItem(1))\n",
    "\n",
    "    # Select subset of original dataframe.\n",
    "    df2 = df.select([\"ident\",\n",
    "            \"iata_code\",\n",
    "            \"name\",\n",
    "            \"type\",\n",
    "            \"municipality\",\n",
    "            \"state\",\n",
    "            \"local_code\",\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "            \"elevation_ft\"]).where(df.iso_country==\"US\")\n",
    "    \n",
    "    # Revise numeric values data types\n",
    "    df2 = df2.withColumn(\"latitude\", df2.latitude.cast('float')) \\\n",
    "            .withColumn(\"longitude\", df2.longitude.cast('float')) \\\n",
    "            .withColumn(\"elevation_fit\", df2.elevation_ft.cast('integer'))\n",
    "    \n",
    "    # Sort \n",
    "    df2 = df2.orderBy([\"state\", \"iata_code\"]) \\\n",
    "            .na.drop(subset='iata_code')\n",
    "    \n",
    "    # Export data to a parquet file\n",
    "    df2.write.partitionBy(\"state\").mode('overwrite').parquet(os.path.join(output_data, \"airports\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "867d106b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fd3fa3ecd34e0b803e0f50ed55730f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_cities_demographics_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Loads and processes the raw U.S. cities demographics data using Spark. \n",
    "    Returns the data as a semi-schematized parquet file.\n",
    "\n",
    "    \"\"\"\n",
    "    df = spark.read.option('header', True) \\\n",
    "                .option('delimiter', \";\") \\\n",
    "                .csv(input_data)\n",
    "    \n",
    "    df2 = df\n",
    "\n",
    "    for original, revised in config.USA_CITIES_RENAME_COLS.items():\n",
    "        df2 = df2.withColumnRenamed(original, revised)\n",
    "\n",
    "    df2 = df2.withColumn(\"state_city\", F.concat_ws(\"_\", df2.state_code, df2.city))\n",
    "\n",
    "    # Change data types to integers\n",
    "    for i_var in config.USA_CITIES_INTEGER_VARS:\n",
    "        df2 = df2.withColumn(i_var, df2[i_var].cast('integer'))\n",
    "    \n",
    "    # Change data types to floats\n",
    "    for f_var in config.USA_CITIES_FLOAT_VARS:\n",
    "        df2 = df2.withColumn(f_var, df2[f_var].cast('float'))\n",
    "\n",
    "    df2 = df2.dropDuplicates([\"state_city\"])\n",
    "\n",
    "    # Create race population group-by table.\n",
    "    race  = df2.select(\"state_city\", \"race\", \"race_pop\")\n",
    "    race = race.groupBy(\"state_city\").pivot(\"race\").agg(F.first(\"race_pop\"))\n",
    "\n",
    "    # join dataframes\n",
    "    df3 = df2.join(race, df2.state_city == race.state_city)\n",
    "    df3 = df3.drop(\"race\", \"race_pop\", \"state_city\", \"state_city\")\n",
    "             \n",
    "    for original, revised in config.RACE_RENAME_COLS.items():\n",
    "        df3 = df3.withColumnRenamed(original, revised)\n",
    "\n",
    "    df3 = df3.orderBy([\"state\", \"city\"])\n",
    "\n",
    "    # Export data to a parquet file\n",
    "    df3.write.partitionBy(\"state\").mode('overwrite').parquet(os.path.join(output_data, \"cities_demographics\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd6573cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9f0f0b474140c0ae12468a82c8eb86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_usa_temperature_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Loads global temperature files. \n",
    "    Returns a parquet file for the climate of U.S. cities.\n",
    "    \"\"\"\n",
    "\n",
    "    df = spark.read.option('header', True).csv(input_data)\n",
    "    df2 = df.select(\"*\").where((df.Country == \"United States\") & (df.dt > \"1969-12-31\"))\n",
    "    \n",
    "    for original, revised in config.TEMPERATURE_RENAME_COLS.items():\n",
    "        df2 = df2.withColumnRenamed(original, revised)\n",
    "    \n",
    "    df2 = df2.withColumn(\"lat_length\", F.length(\"latitude\")) \\\n",
    "            .withColumn(\"long_length\", F.length(\"longitude\")) \\\n",
    "            .withColumn(\"latitude_2\", F.expr(\"\"\"substr(latitude, 1, lat_length-1)\"\"\")) \\\n",
    "            .withColumn(\"longitude_2\", F.expr(\"\"\"substr(longitude, 1, long_length-1)\"\"\"))\n",
    "    \n",
    "    df2 = df2.withColumn(\"latitude\", df2.latitude_2.cast('float')) \\\n",
    "            .withColumn(\"longitude\", df2.longitude_2.cast('float'))\n",
    "\n",
    "    df2 = df2.withColumn(\"longitude\", -1 * col(\"longitude\")) \\\n",
    "            .withColumn(\"year\", year(df2.date)) \\\n",
    "            .withColumn(\"month\", month(df2.date)) \\\n",
    "            .drop(\"Country\", \"lat_length\", \"long_length\", \"latitude_2\", \"longitude_2\")\n",
    "    \n",
    "    df2 = df2.orderBy([\"date\", \"city\"])\n",
    "\n",
    "    df2.write.partitionBy(\"year\").mode('overwrite').parquet(os.path.join(output_data, \"usa_temperatures\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1faa3712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a16d0e7cc4435eb433923a236dcae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_datetime(num_days):\n",
    "    \"\"\"\n",
    "    Converts a uni-codic numeric string value to a date object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start = datetime(1960, 1, 1)\n",
    "        res = start + timedelta(days=int(float((num_days))))\n",
    "        return res.date()\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d047dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4830abae333c4db9a2a64c0f9ff7e4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_usa_tourism_data(spark, tourism_data, airport_codes, country_codes, output_data):\n",
    "    \"\"\"\n",
    "    Loads and process the U.S. tourism SAS files.\n",
    "    Joins tourism data with airport_codes and countries.\n",
    "    Returns PySpark dataframe as partitioned parquet files.\n",
    "    \"\"\"\n",
    "\n",
    "    tourism = spark.read.parquet(tourism_data)\n",
    "    \n",
    "    airports = spark.read.option('header', True).csv(airport_codes)\n",
    "    countries = spark.read.option('header', True).csv(country_codes)\n",
    "\n",
    "    # Create airport-cities dictionary\n",
    "    airports2 = airports.withColumn(\"city\", F.split(col(\"airport\"), \",\").getItem(0))\n",
    "    airports2 = airports2.withColumn(\"city\", F.initcap(\"city\")) \\\n",
    "                        .drop(\"airport\")\n",
    "    \n",
    "    # Create country-I94 code dictionary\n",
    "    udf_datetime_from_sas = udf(lambda x: convert_datetime(x), DateType())\n",
    "    countries2 = countries.withColumn(\"country\", F.initcap(\"country\")) \\\n",
    "                        .withColumn(\"country_code\", countries.country_code.cast('integer'))\n",
    "\n",
    "    # Process tourism data\n",
    "    udf_datetime_from_sas = udf(lambda x: convert_datetime(x), DateType())\n",
    "\n",
    "    #tourism2 = tourism.withColumn(\"arrival_date\", udf_datetime_from_sas(col(\"arrdate\"))) \\\n",
    "    tourism2 = tourism.withColumn(\"arrival_date\", udf_datetime_from_sas(tourism.arrdate)) \\\n",
    "                .withColumn(\"departure_date\", udf_datetime_from_sas(tourism.depdate)) \\\n",
    "                .drop(*config.DROP_TOURISM_COLS)\n",
    "    \n",
    "    for original, renamed in config.TOURISM_RENAME_COLS.items():\n",
    "        tourism2 = tourism2.withColumnRenamed(original, renamed)\n",
    "\n",
    "    for feature in config.TOURISM_INTEGER_VARS:\n",
    "        tourism2 = tourism2.withColumn(feature, tourism2[feature].cast('integer'))\n",
    "    \n",
    "    # Create master dataframe by joining tourism2 and countries2 dataframes.\n",
    "    master = tourism2.join(countries2,\n",
    "                    tourism2.citizen_cntry_code == countries2.country_code,\n",
    "                    how ='left')\n",
    "    \n",
    "    master = master.withColumnRenamed(\"country\", \"citizen_country\") \\\n",
    "                    .drop(\"country_code\")\n",
    "    \n",
    "    master = master.join(countries2,\n",
    "                    master.residency_cntry_code == countries2.country_code,\n",
    "                    how='left')\n",
    "    \n",
    "    master = master.withColumnRenamed(\"country\", \"residency_country\") \\\n",
    "                    .drop(\"country_code\")\n",
    "    \n",
    "    # Join master and airports2 dataframes.\n",
    "    master = master.join(airports2, master.airport == airports2.airport_code, how='left')\n",
    "    master = master.withColumnRenamed(\"city\", \"airport_city\") \\\n",
    "                    .drop(\"airport_code\")\n",
    "\n",
    "    # Change the categorical values from integers/characters to descriptive strings.\n",
    "    travel_mode_func = udf(lambda x: config.MODE_OF_TRAVEL.get(x), StringType())\n",
    "    travel_reason_func = udf(lambda x: config.REASON_FOR_TRAVEL.get(x), StringType())\n",
    "    maritime_signals_func = udf(lambda x: config.MARITIME_SIGNAL_FLAGS.get(x), StringType())\n",
    "\n",
    "    master = master.withColumn(\"travel_mode\", travel_mode_func(master.travel_mode)) \\\n",
    "                        .withColumn(\"reason_for_travel\", travel_reason_func(master.reason_for_travel)) \\\n",
    "                        .withColumn(\"maritime_status_arrival\", maritime_signals_func(master.entdepa)) \\\n",
    "                        .withColumn(\"maritime_status_departure\", maritime_signals_func(master.entdepd)) \\\n",
    "                        .drop(\"entedepa\", \"entdepd\")\n",
    "    \n",
    "    # Add sequential ID\n",
    "    master = master.orderBy(\"arrival_date\") \\\n",
    "                .withColumn(\"tourism_id\", monotonically_increasing_id())\n",
    "    master = master.orderBy(\"tourism_id\")\n",
    "    #window = Window.orderBy(col(\"mono_increasing_id\"))\n",
    "\n",
    "    # Write master dataframe to parque files partitioned by year and month\n",
    "    master.write.partitionBy(\"arrival_yr\", \"arrival_month\").mode('overwrite').parquet(os.path.join(output_data, \"tourist_visits\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed675121",
   "metadata": {},
   "source": [
    "## Run ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee26894f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6eb07901574ff296d883410956690d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = create_spark_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "145a084c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8698c5cd0b864957ad3dad97ab116d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "sc.addPyFile(path=\"s3://aws-emr-resources-588336835903-us-west-2/notebooks/config.py\")\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "335444fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c292825fb514f238011e18b6a52e4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AIRPORT_DATA=\"s3a://udac-capstone/airports.csv\"\n",
    "TOURISM_DATA=\"s3a://udac-capstone/tourism_data/*.parquet\"\n",
    "USA_CITIES_DATA=\"s3a://udac-capstone/us_cities_demographics.csv\"\n",
    "WEATHER_DATA=\"s3a://udac-capstone/GlobalLandTemperaturesByCity.csv\"\n",
    "AIRPORT_CODES=\"s3a://udac-capstone/airport_codes.csv\"\n",
    "COUNTRY_CODES=\"s3a://udac-capstone/country_codes.csv\"\n",
    "OUTPUT_PATH = \"s3a://udac-capstone-output/\"\n",
    "\n",
    "process_airports_data(spark, AIRPORT_DATA, OUTPUT_PATH)\n",
    "process_cities_demographics_data(spark, USA_CITIES_DATA, OUTPUT_PATH)\n",
    "process_usa_temperature_data(spark, WEATHER_DATA, OUTPUT_PATH)\n",
    "process_usa_tourism_data(spark, TOURISM_DATA, AIRPORT_CODES, COUNTRY_CODES, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee7e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
